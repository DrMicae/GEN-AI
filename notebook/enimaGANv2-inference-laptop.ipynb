{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "574a6a46-5c95-4282-a8be-091cfdcb8431",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'e:\\\\GEN-AI\\\\notebook'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa650a61-51f2-4555-8173-c3af2cbdd2e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e:\\GEN-AI\n",
      " Volume in drive E is ubuntu\n",
      " Volume Serial Number is 1E3C-9B9B\n",
      "\n",
      " Directory of e:\\GEN-AI\n",
      "\n",
      "02/03/2025  03:13 PM    <DIR>          .\n",
      "02/02/2025  11:45 AM    <DIR>          .ipynb_checkpoints\n",
      "02/02/2025  11:45 AM    <DIR>          __pycache__\n",
      "01/28/2025  08:00 PM             3,278 animegan.py\n",
      "02/02/2025  11:45 AM    <DIR>          AnimeGANv2\n",
      "02/03/2025  03:13 PM    <DIR>          checkpoints\n",
      "02/03/2025  12:04 PM            84,864 checkpoints.log\n",
      "01/30/2025  10:36 PM    <DIR>          dataset\n",
      "02/03/2025  12:12 PM    <DIR>          frames\n",
      "02/03/2025  12:11 PM    <DIR>          inference-video\n",
      "01/28/2025  08:42 PM               282 main.py\n",
      "02/02/2025  11:53 AM    <DIR>          models\n",
      "02/03/2025  11:50 AM    <DIR>          notebook\n",
      "02/02/2025  02:18 PM    <DIR>          production-video\n",
      "02/03/2025  03:07 PM    <DIR>          temp_frames\n",
      "01/30/2025  11:00 PM     1,634,275,817 temp_frames.zip\n",
      "02/02/2025  11:52 AM    <DIR>          unclassify-file\n",
      "               4 File(s)  1,634,364,241 bytes\n",
      "              13 Dir(s)  149,184,700,416 bytes free\n"
     ]
    }
   ],
   "source": [
    "%cd ..\n",
    "%ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c45e4752-25c2-4ea4-85ab-9d7a3c96a567",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Memory: 15.41 GB\n",
      "Available Memory: 7.91 GB\n",
      "Used Memory: 7.50 GB\n",
      "Memory Usage: 48.7%\n",
      "Total Disk Space: 156.25 GB\n",
      "Used Disk Space: 17.31 GB\n",
      "Free Disk Space: 138.94 GB\n",
      "Disk Usage: 11.1%\n"
     ]
    }
   ],
   "source": [
    "import psutil\n",
    "\n",
    "# Get memory information\n",
    "memory_info = psutil.virtual_memory()\n",
    "print(f\"Total Memory: {memory_info.total / (1024 ** 3):.2f} GB\")\n",
    "print(f\"Available Memory: {memory_info.available / (1024 ** 3):.2f} GB\")\n",
    "print(f\"Used Memory: {memory_info.used / (1024 ** 3):.2f} GB\")\n",
    "print(f\"Memory Usage: {memory_info.percent}%\")\n",
    "\n",
    "# Get disk information\n",
    "disk_info = psutil.disk_usage('/')\n",
    "print(f\"Total Disk Space: {disk_info.total / (1024 ** 3):.2f} GB\")\n",
    "print(f\"Used Disk Space: {disk_info.used / (1024 ** 3):.2f} GB\")\n",
    "print(f\"Free Disk Space: {disk_info.free / (1024 ** 3):.2f} GB\")\n",
    "print(f\"Disk Usage: {disk_info.percent}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c350aa6e-1080-46c7-ba18-1bb37577a37d",
   "metadata": {},
   "source": [
    "%cd AnimeGANv2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "969ba0a2-d3f0-4e2a-b0a7-d1edbc261722",
   "metadata": {},
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "50ea41e4-9c9b-4747-9ebf-fdc01bf8332b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import onnxruntime as ort\n",
    "import typing\n",
    "\n",
    "# Set the environment variable to use the first NVIDIA GPU\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "class AnimeGAN:\n",
    "    def __init__(self, model_path: str, downsize_ratio: float = 1.0, use_gpu: bool = True) -> None:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            model_path: (str) - path to onnx model file\n",
    "            downsize_ratio: (float) - ratio to downsize input frame for faster inference\n",
    "                onnx models:\n",
    "        'https://docs.google.com/uc?export=download&id=1VPAPI84qaPUCHKHJLHiMK7BP_JE66xNe' AnimeGAN_Hayao.onnx\n",
    "        'https://docs.google.com/uc?export=download&id=17XRNQgQoUAnu6SM5VgBuhqSBO4UAVNI1' AnimeGANv2_Hayao.onnx\n",
    "        'https://docs.google.com/uc?export=download&id=10rQfe4obW0dkNtsQuWg-szC4diBzYFXK' AnimeGANv2_Shinkai.onnx\n",
    "        'https://docs.google.com/uc?export=download&id=1X3Glf69Ter_n2Tj6p81VpGKx7U4Dq-tI' AnimeGANv2_Paprika.onnx\n",
    "\n",
    "        \"\"\"\n",
    "        if not os.path.exists(model_path):\n",
    "            raise Exception(f\"Model doesn't exist at {model_path}\")\n",
    "\n",
    "        self.downsize_ratio = downsize_ratio\n",
    "        self.use_gpu = use_gpu\n",
    "\n",
    "        # Check for CUDA availability and set providers\n",
    "        available_providers = ort.get_available_providers()\n",
    "\n",
    "        if self.use_gpu and \"CUDAExecutionProvider\" in available_providers:\n",
    "            # If use_gpu is True and CUDA is available, use CUDA\n",
    "            self.providers = [(\"CUDAExecutionProvider\", {'device_id': 0})]  # Default to GPU 0\n",
    "            print(\"Using GPU for inference.\")\n",
    "        else:\n",
    "            # If use_gpu is False or CUDA is not available, use CPU\n",
    "            self.providers = ['CPUExecutionProvider']\n",
    "            print(\"Using CPU for inference.\")\n",
    "\n",
    "        self.ort_sess = ort.InferenceSession(model_path, providers=self.providers)\n",
    "\n",
    "    def to_32s(self, x):\n",
    "        \"\"\" Ensure input is multiple of 32 \"\"\"\n",
    "        return 256 if x < 256 else x - x % 32\n",
    "\n",
    "    def process_frame(self, frame: np.ndarray, x32: bool = True) -> np.ndarray:\n",
    "        \"\"\" Function to process frame to fit model input as 32 multiplier and resize to fit model input \"\"\"\n",
    "        h, w = frame.shape[:2]\n",
    "        if x32:  # resize image to multiple of 32s\n",
    "            frame = cv2.resize(frame, (self.to_32s(int(w * self.downsize_ratio * 2)), self.to_32s(int(h * self.downsize_ratio * 2))))\n",
    "        frame = frame.astype(np.float32) / 127.5 - 1.0\n",
    "        return frame\n",
    "\n",
    "    def post_process(self, frame: np.ndarray, wh: typing.Tuple[int, int]) -> np.ndarray:\n",
    "        \"\"\" Convert model float output to uint8 image resized to double the original frame size \"\"\"\n",
    "        frame = (frame.squeeze() + 1.) / 2 * 255\n",
    "        frame = frame.astype(np.uint8)\n",
    "        #frame = cv2.resize(frame, (wh[0] * 2, wh[1] * 2)) # resize here\n",
    "        frame = cv2.resize(frame, (wh[0], wh[1]))\n",
    "        return frame\n",
    "\n",
    "    def __call__(self, frame: np.ndarray) -> np.ndarray:\n",
    "        \"\"\" Main function to process each frame \"\"\"\n",
    "        image = self.process_frame(frame)\n",
    "\n",
    "        # Get input name\n",
    "        input_name = self.ort_sess.get_inputs()[0].name\n",
    "\n",
    "        # Run inference\n",
    "        outputs = self.ort_sess.run(None, {input_name: np.expand_dims(image, axis=0)})\n",
    "\n",
    "        # Post-process the output\n",
    "        frame = self.post_process(outputs[0], frame.shape[:2][::-1])\n",
    "        return frame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c123a2-18cb-4108-bd0e-3fce9a636d6d",
   "metadata": {},
   "source": [
    "!pip install --upgrade onnxruntime-gpu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f08619b-8a43-4239-8f13-f96c86d86f1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['TensorrtExecutionProvider', 'CUDAExecutionProvider', 'CPUExecutionProvider']\n",
      "ONNX Runtime version: 1.20.1\n"
     ]
    }
   ],
   "source": [
    "import onnxruntime as ort\n",
    "print(ort.get_available_providers())\n",
    "print(\"ONNX Runtime version:\", ort.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5512587c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 0: NVIDIA GeForce RTX 3050 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import GPUtil\n",
    "\n",
    "gpus = GPUtil.getGPUs()\n",
    "for gpu in gpus:\n",
    "    print(f\"GPU {gpu.id}: {gpu.name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ac14b6b8-6d05-4c43-81cd-16333df7926d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "\n",
    "# Function to split video into frames\n",
    "def split_video_to_frames(video_path, output_folder):\n",
    "    # Create the output folder if it doesn't exist\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    # Open the video file\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "    # Check if the video file opened successfully\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Could not open video.\")\n",
    "        return\n",
    "\n",
    "    frame_count = 0\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Save the frame as an image file\n",
    "        frame_filename = os.path.join(output_folder, f'frame_{frame_count:04d}.jpg')\n",
    "        cv2.imwrite(frame_filename, frame)\n",
    "\n",
    "        frame_count += 1\n",
    "\n",
    "    # Release the video capture object\n",
    "    cap.release()\n",
    "    print(f\"Video split into {frame_count} frames and saved in '{output_folder}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "503e5564",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_directory_if_not_exists(directory):\n",
    "    \"\"\"Create a directory if it does not already exist.\"\"\"\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "def create_file_if_not_exists(file_path):\n",
    "    \"\"\"Create a file if it does not already exist.\"\"\"\n",
    "    if not os.path.exists(file_path):\n",
    "        open(file_path, 'w').close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4c23659f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "video_name = '1stWorldTaijiquanChampionships'\n",
    "video_path = f'inference-video/{video_name}.webm'\n",
    "output_folder = f'frames/{video_name}_frames'\n",
    "temp_output_folder = f'temp_frames/{video_name}_temp_frames'\n",
    "checkpoints_log_path = f'checkpoints/{video_name}_checkpoints.log'\n",
    "\n",
    "# Ensure the directories and the log file exist\n",
    "create_directory_if_not_exists(output_folder)\n",
    "create_directory_if_not_exists(temp_output_folder)\n",
    "create_directory_if_not_exists(os.path.dirname(checkpoints_log_path))\n",
    "create_file_if_not_exists(checkpoints_log_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0feb345e",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_video_to_frames(video_path, output_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "37b1af69-cb99-40df-9438-acb72e9e5f09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted 0 filenames into checkpoints.log\n",
      "Ordered and deduplicated checkpoint names in 'checkpoints/1stWorldTaijiquanChampionships_checkpoints.log'.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def get_sorted_files_from_folder(folder):\n",
    "    \"\"\"Retrieve and sort file names from a specified folder.\"\"\"\n",
    "    files = [f for f in os.listdir(folder) if os.path.isfile(os.path.join(folder, f))]\n",
    "    return sorted(files)\n",
    "\n",
    "# Get a list of all files in temp_output_folder\n",
    "files_in_temp_output_folder = [f for f in os.listdir(temp_output_folder) if os.path.isfile(os.path.join(temp_output_folder, f))]\n",
    "\n",
    "# Append each filename to checkpoints.log\n",
    "checkpoints_log_path = f'checkpoints/{video_name}_checkpoints.log'\n",
    "\n",
    "# Load the checkpoint log and get the list of already processed files\n",
    "if os.path.exists(checkpoints_log_path):\n",
    "    with open(checkpoints_log_path, 'r') as f:\n",
    "        processed_files = f.read().splitlines()\n",
    "else:\n",
    "    processed_files = []\n",
    "\n",
    "with open(checkpoints_log_path, 'a') as f:\n",
    "    for filename in files_in_temp_output_folder:\n",
    "        if filename not in processed_files:\n",
    "            f.write(f\"{filename}\\n\")\n",
    "            processed_files.append(filename)\n",
    "\n",
    "print(f\"Inserted {len(files_in_temp_output_folder)} filenames into checkpoints.log\")\n",
    "\n",
    "def order_and_deduplicate_checkpoint_names(checkpoint_file):\n",
    "    \"\"\"Orders and deduplicates checkpoint names in the given file.\n",
    "\n",
    "    Args:\n",
    "        checkpoint_file: The path to the checkpoint file.\n",
    "    \"\"\"\n",
    "    with open(checkpoint_file, 'r') as f:\n",
    "        checkpoint_names = f.read().splitlines()\n",
    "\n",
    "    # Remove empty lines\n",
    "    checkpoint_names = [name for name in checkpoint_names if name]\n",
    "\n",
    "    # Order the names\n",
    "    checkpoint_names.sort()\n",
    "\n",
    "    # Deduplicate the names\n",
    "    unique_checkpoint_names = []\n",
    "    for name in checkpoint_names:\n",
    "        if name not in unique_checkpoint_names:\n",
    "            unique_checkpoint_names.append(name)\n",
    "\n",
    "    # Write the unique names back to the file\n",
    "    with open(checkpoint_file, 'w') as f:\n",
    "        for name in unique_checkpoint_names:\n",
    "            f.write(f\"{name}\\n\")\n",
    "\n",
    "    print(f\"Ordered and deduplicated checkpoint names in '{checkpoint_file}'.\")\n",
    "\n",
    "# Call the function with the path to your checkpoint file\n",
    "checkpoint_file = f'checkpoints/{video_name}_checkpoints.log'\n",
    "order_and_deduplicate_checkpoint_names(checkpoint_file)\n",
    "\n",
    "# Get and print sorted files from both output folders\n",
    "sorted_output_files = get_sorted_files_from_folder(output_folder)\n",
    "sorted_temp_output_files = get_sorted_files_from_folder(temp_output_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b8211fe1-9312-4252-9efe-c70c5afcd8fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU for inference.\n"
     ]
    }
   ],
   "source": [
    "# Initialize the AnimeGAN model with GPU1 support\n",
    "model_name = \"Shinkai_53.onnx\"\n",
    "model_path = f\"models/AnimeGAN/{model_name}\"\n",
    "#animegan = AnimeGAN(model_path, device_id=1)\n",
    "#Manual GPU or CPU\n",
    "animegan = AnimeGAN(model_path, use_gpu=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd105fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "session = ort.InferenceSession(model_path, providers=['CUDAExecutionProvider'])\n",
    "num_gpus = len(session.get_providers())\n",
    "print(\"Number of GPUs available:\", num_gpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "327b6802-fe75-43d8-ad72-1664e86b8483",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "RuntimeException",
     "evalue": "[ONNXRuntimeError] : 6 : RUNTIME_EXCEPTION : Non-zero status code returned while running Pad node. Name:'generator/G_MODEL/E/MirrorPad' Status Message: D:\\a\\_work\\1\\s\\onnxruntime\\core\\framework\\bfc_arena.cc:376 onnxruntime::BFCArena::AllocateRawInternal Failed to allocate memory for requested buffer of size 4221405184\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeException\u001b[0m                          Traceback (most recent call last)",
      "File \u001b[1;32m<timed exec>:58\u001b[0m\n",
      "File \u001b[1;32m<timed exec>:39\u001b[0m, in \u001b[0;36mprocess_images\u001b[1;34m(input_folder, output_folder, checkpoint_file)\u001b[0m\n",
      "Cell \u001b[1;32mIn[4], line 71\u001b[0m, in \u001b[0;36mAnimeGAN.__call__\u001b[1;34m(self, frame)\u001b[0m\n\u001b[0;32m     68\u001b[0m input_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mort_sess\u001b[38;5;241m.\u001b[39mget_inputs()[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mname\n\u001b[0;32m     70\u001b[0m \u001b[38;5;66;03m# Run inference\u001b[39;00m\n\u001b[1;32m---> 71\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mort_sess\u001b[38;5;241m.\u001b[39mrun(\u001b[38;5;28;01mNone\u001b[39;00m, {input_name: np\u001b[38;5;241m.\u001b[39mexpand_dims(image, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)})\n\u001b[0;32m     73\u001b[0m \u001b[38;5;66;03m# Post-process the output\u001b[39;00m\n\u001b[0;32m     74\u001b[0m frame \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_process(outputs[\u001b[38;5;241m0\u001b[39m], frame\u001b[38;5;241m.\u001b[39mshape[:\u001b[38;5;241m2\u001b[39m][::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\onnxruntime\\capi\\onnxruntime_inference_collection.py:266\u001b[0m, in \u001b[0;36mSession.run\u001b[1;34m(self, output_names, input_feed, run_options)\u001b[0m\n\u001b[0;32m    264\u001b[0m     output_names \u001b[38;5;241m=\u001b[39m [output\u001b[38;5;241m.\u001b[39mname \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_outputs_meta]\n\u001b[0;32m    265\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 266\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sess\u001b[38;5;241m.\u001b[39mrun(output_names, input_feed, run_options)\n\u001b[0;32m    267\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m C\u001b[38;5;241m.\u001b[39mEPFail \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m    268\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enable_fallback:\n",
      "\u001b[1;31mRuntimeException\u001b[0m: [ONNXRuntimeError] : 6 : RUNTIME_EXCEPTION : Non-zero status code returned while running Pad node. Name:'generator/G_MODEL/E/MirrorPad' Status Message: D:\\a\\_work\\1\\s\\onnxruntime\\core\\framework\\bfc_arena.cc:376 onnxruntime::BFCArena::AllocateRawInternal Failed to allocate memory for requested buffer of size 4221405184\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import onnxruntime as ort\n",
    "import os\n",
    "import cv2\n",
    "import time\n",
    "import numpy as np\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import torch\n",
    "\n",
    "# 1. Define all the helper functions (sub-functions) first\n",
    "def clear_gpu_memory():\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()  # Release unused GPU memory\n",
    "\n",
    "# Function to process images in a folder and save to a new folder with the same filenames\n",
    "def process_images(input_folder, output_folder, checkpoint_file):\n",
    "    # Create the output folder if it doesn't exist\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    # Load the checkpoint log and get the list of already processed files\n",
    "    if os.path.exists(checkpoint_file):\n",
    "        with open(checkpoint_file, 'r') as f:\n",
    "            processed_files = f.read().splitlines()\n",
    "    else:\n",
    "        processed_files = []\n",
    "\n",
    "    # List all files in the input folder\n",
    "    input_files = sorted([f for f in os.listdir(input_folder) if os.path.isfile(os.path.join(input_folder, f))])\n",
    "\n",
    "    # Process only files that exist in the checkpoint log\n",
    "    for input_file in input_files:\n",
    "        # Start time\n",
    "        start_time = time.time()\n",
    "        if input_file not in processed_files:\n",
    "            # Read the image\n",
    "            input_path = os.path.join(input_folder, input_file)\n",
    "            frame = cv2.imread(input_path)\n",
    "\n",
    "            # Process the image with AnimeGAN\n",
    "            processed_frame = animegan(frame)\n",
    "\n",
    "            # Save the processed image to the output folder with the same filename\n",
    "            output_path = os.path.join(output_folder, input_file)\n",
    "            cv2.imwrite(output_path, processed_frame)\n",
    "\n",
    "            # Update the checkpoint log\n",
    "            with open(checkpoint_file, 'a') as f:\n",
    "                f.write(f\"{input_file}\\n\")\n",
    "            \n",
    "            # End time\n",
    "            end_time = time.time()\n",
    "            print(f\"Processed '{input_file}' and saved to '{output_folder}'.\")\n",
    "            # Calculate duration\n",
    "            duration = end_time - start_time\n",
    "            print(f\"Inference took {duration:.2f} seconds\")\n",
    "        clear_gpu_memory()\n",
    "\n",
    "# Run the image processing with the ONNX model and basic optimization\n",
    "process_images(output_folder, temp_output_folder, checkpoints_log_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89dc2565-3c0f-4606-8596-51a068981df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "from moviepy import VideoFileClip\n",
    "\n",
    "# Function to merge images into a video\n",
    "def merge_images_to_video(input_folder, output_video_path, frame_rate, frame_width, frame_height):\n",
    "    # List all files in the input folder and sort them by name (or modify the sorting logic as necessary)\n",
    "    input_files = sorted([f for f in os.listdir(input_folder) if f.endswith(('.jpg', '.png', '.jpeg'))])\n",
    "\n",
    "    if not input_files:\n",
    "        print(\"No image files found in the input folder.\")\n",
    "        return\n",
    "\n",
    "    # Define the codec and create VideoWriter object\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'XVID')  # You can choose 'H264' if needed\n",
    "    out = cv2.VideoWriter(output_video_path, fourcc, frame_rate, (frame_width, frame_height))\n",
    "\n",
    "    for input_file in input_files:\n",
    "        input_path = os.path.join(input_folder, input_file)\n",
    "        frame = cv2.imread(input_path)\n",
    "\n",
    "        # Ensure the frame is valid\n",
    "        if frame is None:\n",
    "            print(f\"Error reading {input_file}. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        # Resize the frame to match the video dimensions\n",
    "        frame_resized = cv2.resize(frame, (frame_width, frame_height))\n",
    "\n",
    "        # Write the resized frame to the video\n",
    "        out.write(frame_resized)\n",
    "\n",
    "    # Release the video writer\n",
    "    out.release()\n",
    "    print(f\"Video created and saved to '{output_video_path}'.\")\n",
    "\n",
    "# Set the video name\n",
    "video_name = 'dance1'\n",
    "\n",
    "# Paths for input and output\n",
    "video_path = f'{video_name}.mp4'  # Original video path\n",
    "output_video_path = f'{video_name}_output.mp4'  # New video after merging images\n",
    "\n",
    "# Get properties from the original video\n",
    "cap = cv2.VideoCapture(video_path)  # Open the original video to extract properties\n",
    "frame_rate = cap.get(cv2.CAP_PROP_FPS)\n",
    "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "cap.release()  # Release after getting properties\n",
    "\n",
    "# Folder containing frames to merge into video\n",
    "input_folder = 'temp_frames'  # Make sure this folder contains the frames you want to merge\n",
    "\n",
    "# Merge images into video\n",
    "merge_images_to_video(input_folder, output_video_path, frame_rate, frame_width, frame_height)\n",
    "\n",
    "# Now, using moviepy to add the audio from the original video\n",
    "final_video_path = f'{video_name}_final_with_audio.mp4'  # Final output video path\n",
    "\n",
    "# Load original video to extract audio\n",
    "original_video = VideoFileClip(video_path)\n",
    "original_audio = original_video.audio\n",
    "\n",
    "# Load the newly created video\n",
    "new_video = VideoFileClip(output_video_path)\n",
    "\n",
    "# Set the original audio to the new video\n",
    "final_video = new_video.with_audio(original_audio)\n",
    "\n",
    "# Write the final video to a file with audio; 8000k (which is 8 Mbps). This is a good middle ground for Full HD (1080p) video quality\n",
    "final_video.write_videofile(final_video_path, codec='libx264', audio_codec='aac', bitrate='8000k')\n",
    "\n",
    "print(f\"Final video created and saved to '{final_video_path}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14ea30a-b2ea-4038-81f3-2b230728f629",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyba",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
